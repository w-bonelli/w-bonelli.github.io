---
layout: post
title: functions of loss
---

Computer scientists and AI researchers from Minsky to Bostrom have considered the dangers of motivated and highly intelligent non-human agents. We have lived among these for most, if not all, of recorded human history. What is a state, a corporation, a religion, or your local chapter of the Rotary Club if not a suprahuman intelligence? We need not wait for some proto-AGI with an insufficiently nuanced objective function to observe the influence of organisms larger than ourselves on our lives.

Why do so few eminent technologists talk about this? Have they never read Gibson or Lem? Have they never leafed through a cybernetics (or even an ecology) text? Do they find obfuscation conducive to career success, or think their meaning clear despite its deliberately ambiguous object?

One function of loss, in human and non-human intelligences alike, is to recalibrate models to the constraints imposed by reality. We have an intuitive and effortless grasp of some of these: we know we cannot walk through walls. We seem much less likely to recognize other constraint classes: those spanning much smaller or larger spatio-temporal scales than we, for instance; or those which, as Lippman put it a century ago, threaten "the security of that to which we've given our allegiance".[^1]

I'm no better. Who can resist attachment to the things one sacrifices for and advances within? Who can live in the long now, immersed in the smooth and seamless hum of the moment? Loss functions are no use until you lose.

---

[^1]: Lippmann, W. (1919). The Basic Problem of Democracy. The Atlantic, Atlantic Media Company. <a href="www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/">www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/</a>. 
