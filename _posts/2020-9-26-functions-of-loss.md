---
layout: post
title: functions of loss
---

Technologists from Minsky to Musk have considered questions of incentive misalignment in non-human agents: Bostrom's [paperclip maximizer](https://nickbostrom.com/ethics/ai.html) may be the best known example. "Artificial" super-intelligence is typically termed hypothetical, but the gods have lived among us for most, if not all, of recorded human history. What is a state, a corporation, an organized religion, a cultural or academic institution, or your local chapter of the Rotary Club if not a suprahuman intelligence? We need not wait for some proto-AGI with an insufficiently nuanced objective function to observe the influence of organisms larger than ourselves on our lives.

One function of loss, in human and non-human intelligences alike, is to recalibrate models to the constraints imposed by reality. We have an intuitive and effortless grasp of some of these: we know we cannot walk through walls. We seem much less likely to recognize other constraint classes: those spanning much smaller or larger spatio-temporal scales than we, for instance; or those which, as Lippman put it a century ago, threaten "the security of that to which we've given our allegiance".[^1]

Who can resist attachment to the things one sacrifices for and advances within? Who can live in the long now, immersed in the seamless hum of the moment? Loss functions are no use until you lose.

---

[^1]: Lippmann, W. (1919). The Basic Problem of Democracy. The Atlantic, Atlantic Media Company. <a href="www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/">www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/</a>. 
