---
layout: post
title: functions of loss
---

Technologists from Minsky to Musk have considered questions of incentive misalignment in artificial agents, often with some urgency: Bostrom's [paperclip maximizer](https://nickbostrom.com/ethics/ai.html) is one of the better known framings of the problem. Still, super-intelligence is termed hypothetical. Somehow most agree that, though we may realize these risks "soon" (how soon varies, depending who is asked), we have not yet breached the proverbial seal.

One function of loss, among humans and non-humans alike, is to recalibrate models to the constraints imposed by reality. A dog needs one collision with a screen door to internalize the unreliability of its vision. Human beings tend to avoid this fate, but it remains difficult for us to deciper different classes of objects: for instance, as Lippman put it a century ago, those which

> impugn the security of that to which we've given our allegiance.[^1]

We need not wait for generally intelligent software shackled to a badly nuanced objective function to witness "intelligences" larger than ourselves. What is Google, the Executive Office of the President of the United States, or the Centers for Disease Control and Prevention if not suprahuman agents? That these things string humans together in causal chains rather than cells or transistors is immaterial.

What do they maximize? It might as well be paperclips. It seems we live in a surveillance state incapable of controlling a pandemic, and the planet grows less hospitable by the year. Gould and Lewontin warn:

> Human cultural practices can be orthogenetic and drive toward extinction in ways that Darwinian processes, based on genetic selection, cannot.[^2]

Having sacrificed for and advanced within some social structure or milieu, who can resist attachment to it? Who can live in the long now, immersed in the seamless hum of the moment? Loss functions are no use until you lose.

---

[^1]: Lippmann, W. (1919). The Basic Problem of Democracy. The Atlantic, Atlantic Media Company. <a href="www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/">www.theatlantic.com/magazine/archive/1919/11/the-basic-problem-of-democracy/569095/</a>. 

[^2]: Gould, S. J., and R. Lewontin. (1979). The spandrels of San Marco and the Panglossian paradigm: A critique of the adaptationist programme. Proceedings of the Royal Society of London. <a href="https://doi.org/10.1098/rspb.1979.0086">https://doi.org/10.1098/rspb.1979.0086</a>

